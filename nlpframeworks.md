# NLP Frameworks

## AllenNLP
<p><span class="col-11 text-gray-dark mr-2">An open-source NLP research library, built on PyTorch. AllenNLP is a NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. AllenNLP makes it easy to design and evaluate new deep learning models for nearly any NLP problem, along with the infrastructure to easily run them in the cloud or on your laptop.<br />
</span></p>
<p>AllenNLP was designed with the following principles:</p>
<ul>
<li><em>Hyper-modular and lightweight.</em> Use the parts which you like seamlessly with PyTorch.</li>
<li><em>Extensively tested and easy to extend.</em> Test coverage is above 90% and the example models provide a template for contributions.</li>
<li><em>Take padding and masking seriously</em>, making it easy to implement correct models without the pain.</li>
<li><em>Experiment friendly.</em> Run reproducible experiments from a json specification with comprehensive logging.</li>
</ul>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [http://allennlp.org/](http://allennlp.org/)
**Source Location** | [https://github.com/allenai/allennlp](https://github.com/allenai/allennlp)
*Tag(s)* |ML, NLP, Python


## Apache OpenNLP
<p>The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.</p>
<p>The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also included maximum entropy and perceptron based machine learning.</p>
<p>The goal of the OpenNLP project will be to create a mature toolkit for the abovementioned tasks. An additional goal is to provide a large number of pre-built models for a variety of languages, as well as the annotated text resources that those models are derived from.</p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Java
**Project URL** | [http://opennlp.apache.org/](http://opennlp.apache.org/)
**Source Location** | [http://opennlp.apache.org/source-code.html](http://opennlp.apache.org/source-code.html)
*Tag(s)* |NLP


## Apache Tika
<p>The Apache Tikaâ„¢ toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). All of these file types can be parsed through a single interface, making Tika useful for search engine indexing, content analysis, translation, and much more.</p>
<p>Several wrappers are available to use Tika in another programming language, such as <a class="externalLink" href="https://github.com/aviks/Taro.jl">Julia</a> or <a class="externalLink" href="https://github.com/chrismattmann/tika-python">Python</a></p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Java
**Project URL** | [https://tika.apache.org/](https://tika.apache.org/)
**Source Location** | [https://tika.apache.org/](https://tika.apache.org/)
*Tag(s)* |NLP


## BERT

<p><strong>BERT</strong>, or <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from
<strong>T</strong>ransformers, is a new method of pre-training language representations which
obtains state-of-the-art results on a wide array of Natural Language Processing
(NLP) tasks.</p>



<p>Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.</p>



<p>OSS NLP training models from Google Research.</p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [https://github.com/google-research/bert](https://github.com/google-research/bert)
**Source Location** | [https://github.com/google-research/bert](https://github.com/google-research/bert)
*Tag(s)* |NLP


## Bling Fire

<p>A lightning fast Finite State machine and REgular expression manipulation library. Bling Fire Tokenizer is a tokenizer designed for fast-speed and quality tokenization of Natural Language text. It mostly follows the tokenization logic of NLTK, except hyphenated words are split and a few errors are fixed.</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | CPP
**Project URL** | [https://github.com/Microsoft/BlingFire](https://github.com/Microsoft/BlingFire)
**Source Location** | [https://github.com/Microsoft/BlingFire](https://github.com/Microsoft/BlingFire)
*Tag(s)* |NLP


## ERNIE

<p> An Implementation of ERNIE For Language Understanding (including Pre-training models and Fine-tuning tools) </p>



<p> <strong><a href="https://arxiv.org/abs/1907.12412v1">ERNIE 2.0</a> is a continual pre-training framework for language understanding</strong>  in which pre-training tasks can be incrementally built and learned  through multi-task learning. In this framework, different customized  tasks can be incrementally introduced at any time. For example, the  tasks including named entity prediction, discourse relation recognition,  sentence order prediction are leveraged in order to enable the models  to learn language representations. </p>



<p></p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [https://github.com/PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
**Source Location** | [https://github.com/PaddlePaddle/ERNIE](https://github.com/PaddlePaddle/ERNIE)
*Tag(s)* |NLP, Python


## fastText

<p><a href="https://fasttext.cc/">fastText</a> is a library for efficient learning of word representations and sentence classification. Models can later be reduced in size to even fit on mobile devices. </p>



<p>Created by Facebook Opensource, now available for us all. Also used for the new search on StackOverflow, see <a href="https://stackoverflow.blog/2019/08/14/crokage-a-new-way-to-search-stack-overflow/" target="_blank" rel="noreferrer noopener" aria-label="https://stackoverflow.blog/2019/08/14/crokage-a-new-way-to-search-stack-overflow/ (opens in a new tab)">https://stackoverflow.blog/2019/08/14/crokage-a-new-way-to-search-stack-overflow/</a></p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | CPP, Python
**Project URL** | [https://fasttext.cc/](https://fasttext.cc/)
**Source Location** | [https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)
*Tag(s)* |NLP


## Flair

<p>A very simple framework for <strong>state-of-the-art NLP</strong>. Developed by <a href="https://research.zalando.com/">Zalando Research</a>.</p>



<p>Flair is:</p>



<ul><li>
<strong>A powerful NLP library.</strong> Flair allows you to apply our state-of-the-art natural language processing (NLP)
models to your text, such as named entity recognition (NER), part-of-speech tagging (PoS),
sense disambiguation and classification.
</li><li>
<strong>Multilingual.</strong> Thanks to the Flair community, we support a rapidly growing number of languages. We also now include
&#8216;<em>one model, many languages</em>&#8216; taggers, i.e. single models that predict PoS or NER tags for input text in various languages.
</li><li>
<strong>A text embedding library.</strong> Flair has simple interfaces that allow you to use and combine different word and
document embeddings, including our proposed <strong><a href="https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view?usp=sharing">Flair embeddings</a></strong>, BERT embeddings and ELMo embeddings.
</li><li>
<strong>A Pytorch NLP framework.</strong> Our framework builds directly on <a href="https://pytorch.org/">Pytorch</a>, making it easy to
train your own models and experiment with new approaches using Flair embeddings and classes.
</li></ul>



<p></p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://github.com/zalandoresearch/flair](https://github.com/zalandoresearch/flair)
**Source Location** | [https://github.com/zalandoresearch/flair](https://github.com/zalandoresearch/flair)
*Tag(s)* |ML, NLP, Python


## Gensim
<p>Gensim is a Python library for <em>topic modelling</em>, <em>document indexing</em> and <em>similarity retrieval</em> with large corpora. Target audience is the <em>natural language processing</em> (NLP) and <em>information retrieval</em> (IR) community.</p>
<p>&#160;</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim)
**Source Location** | [https://github.com/RaRe-Technologies/gensim](https://github.com/RaRe-Technologies/gensim)
*Tag(s)* |ML, NLP, Python


## Icecaps

<p> Microsoft Icecaps is an open-source toolkit for building neural  conversational systems. Icecaps provides an array of tools from recent  conversation modeling and general NLP literature within a flexible  paradigm that enables complex multi-task learning setups. </p>



<p>Background information can be found here <a href="https://www.aclweb.org/anthology/P19-3021" target="_blank" rel="noreferrer noopener" aria-label="https://www.aclweb.org/anthology/P19-3021 (opens in a new tab)">https://www.aclweb.org/anthology/P19-3021</a></p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://www.microsoft.com/en-us/research/project/microsoft-icecaps/](https://www.microsoft.com/en-us/research/project/microsoft-icecaps/)
**Source Location** | [https://github.com/microsoft/icecaps](https://github.com/microsoft/icecaps)
*Tag(s)* |NLP, Python


## jiant

<p><code>jiant</code> is a software toolkit for natural language processing  research, designed to facilitate work on multitask learning and  transfer learning for sentence understanding tasks.</p>



<p>New software for the The General Language Understanding Evaluation (GLUE) benchmark. This software can be used for evaluating, and analyzing natural language understanding systems. </p>



<p>See also: https://super.gluebenchmark.com/</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://jiant.info/](https://jiant.info/)
**Source Location** | [https://github.com/nyu-mll/jiant](https://github.com/nyu-mll/jiant)
*Tag(s)* |NLP, Python, Research


## Klassify

<p>Redis based text classification service with real-time web interface.</p>



<p>What is Text Classification: Text classification, document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories.</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://github.com/fatiherikli/klassify](https://github.com/fatiherikli/klassify)
**Source Location** | [https://github.com/fatiherikli/klassify](https://github.com/fatiherikli/klassify)
*Tag(s)* |ML, NLP, Text classification


## Neuralcoref
<p>State-of-the-art coreference resolution based on neural nets and spaCy.</p>
<p>NeuralCoref is a pipeline extension for spaCy 2.0 that annotates and resolves coreference clusters using a neural network. NeuralCoref is production-ready, integrated in spaCy&#8217;s NLP pipeline and easily extensible to new training datasets.</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://huggingface.co/coref/](https://huggingface.co/coref/)
**Source Location** | [https://github.com/huggingface/neuralcoref](https://github.com/huggingface/neuralcoref)
*Tag(s)* |ML, NLP, Python


## NLP Architect

<p>NLP Architect is an open-source Python library for exploring the state-of-the-art deep learning topologies and techniques for natural language processing and natural language understanding. It is intended to be a platform for future research and collaboration.</p>



<p>Features:</p>



<ul><li>
Core NLP models used in many NLP tasks and useful in many NLP applications
</li><li>
Novel NLU models showcasing novel topologies and techniques
</li><li>
Optimized NLP/NLU models showcasing different optimization algorithms on neural NLP/NLU models
</li><li>
Model-oriented design:
<ul><li>Train and run models from command-line.</li><li>API for using models for inference in python.</li><li>Procedures to define custom processes for training,    inference or anything related to processing.</li><li>CLI sub-system for running procedures</li></ul>
</li><li>
Based on optimized Deep Learning frameworks:
<ul><li><a href="https://www.tensorflow.org/">TensorFlow</a></li><li><a href="https://pytorch.org/">PyTorch</a></li><li><a href="https://dynet.readthedocs.io/en/latest/">Dynet</a></li></ul>
</li><li>
Essential utilities for working with NLP models &#8211; Text/String pre-processing, IO, data-manipulation, metrics, embeddings.
</li></ul>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [http://nlp_architect.nervanasys.com/](http://nlp_architect.nervanasys.com/)
**Source Location** | [https://github.com/NervanaSystems/nlp-architect](https://github.com/NervanaSystems/nlp-architect)
*Tag(s)* |ML, ML Tool, NLP, Python


## NLTK (Natural Language Toolkit)

<p>NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to <a class="reference external" href="http://nltk.org/nltk_data/">over 50 corpora and lexical resources</a> such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries. </p>



<p>NLTK is known as a good learning platform, but is not designed to robustly serve millions of customers. So ideal for experimenting, but for some business use cases not always the right choice.<br></p>



<p>Check also the (free) online Book (OReily published)</p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [http://www.nltk.org](http://www.nltk.org)
**Source Location** | [https://github.com/nltk/nltk](https://github.com/nltk/nltk)
*Tag(s)* |NLP


## Pattern
<p>Pattern is a web mining module for Python. It has tools for:</p>
<ul>
<li>Data Mining: web services (Google, Twitter, Wikipedia), web crawler, HTML DOM parser</li>
<li>Natural Language Processing: part-of-speech taggers, n-gram search, sentiment analysis, WordNet</li>
<li>Machine Learning: vector space model, clustering, classification (KNN, SVM, Perceptron)</li>
<li>Network Analysis: graph centrality and visualization.</li>
</ul>

Item | Value 
----- | -----
**SBB License** | BSD License 2.0 (3-clause, New or Revised) License
**Core Technology** | Python
**Project URL** | [https://www.clips.uantwerpen.be/pages/pattern](https://www.clips.uantwerpen.be/pages/pattern)
**Source Location** | [https://github.com/clips/pattern](https://github.com/clips/pattern)
*Tag(s)* |ML, NLP, Web scraping


## Rant

<p>Rant is an all-purpose procedural text engine that is most simply described as the opposite of Regex. It has been refined to include a dizzying array of features for handling everything from the most basic of string generation tasks to advanced dialogue generation, code templating, automatic formatting, and more.</p>



<p>The goal of the project is to enable developers of all kinds to automate repetitive writing tasks with a high degree of creative freedom.</p>



<p>Features:</p>



<ul><li>Recursive, weighted branching with several selection modes</li><li>Queryable dictionaries</li><li>Automatic capitalization, rhyming, English indefinite articles, and multi-lingual number verbalization</li><li>Print to multiple separate outputs</li><li>Probability modifiers for pattern elements</li><li>Loops, conditional statements, and subroutines</li><li>Fully-functional object model</li><li>Import/Export resources easily with the .rantpkg format</li><li>Compatible with Unity 2017</li></ul>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | .NET
**Project URL** | [https://rant-lang.org/](https://rant-lang.org/)
**Source Location** | [https://github.com/rant-lang/rant](https://github.com/rant-lang/rant)
*Tag(s)* |.NET, ML, NLP, text generation


## SpaCy
<div class="o-grid__col o-grid__col--third">
<p><span class="col-11 text-gray-dark mr-2">Industrial-strength Natural Language Processing (NLP) with Python and Cython</span></p>
<p class="u-heading-2 u-heading">Features:</p>
<ul class="c-list c-list--bullets o-block u-text">
<li class="c-list__item">Non-destructive <strong>tokenization</strong></li>
<li class="c-list__item"><strong>Named entity</strong> recognition</li>
<li class="c-list__item">Support for <strong>26+ languages</strong></li>
<li class="c-list__item"><strong>13 statistical models</strong> for 8 languages</li>
<li class="c-list__item">Pre-trained <strong>word vectors</strong></li>
<li class="c-list__item">Easy <strong>deep learning</strong> integration</li>
<li class="c-list__item">Part-of-speech tagging</li>
<li class="c-list__item">Labelled dependency parsing</li>
<li class="c-list__item">Syntax-driven sentence segmentation</li>
<li class="c-list__item">Built in <strong>visualizers</strong> for syntax and NER</li>
<li class="c-list__item">Convenient string-to-hash mapping</li>
<li class="c-list__item">Export to numpy data arrays</li>
<li class="c-list__item">Efficient binary serialization</li>
<li class="c-list__item">Easy <strong>model packaging</strong> and deployment</li>
<li class="c-list__item">State-of-the-art speed</li>
<li class="c-list__item">Robust, rigorously evaluated accuracy</li>
</ul>
</div>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://spacy.io/](https://spacy.io/)
**Source Location** | [https://github.com/explosion/spaCy](https://github.com/explosion/spaCy)
*Tag(s)* |NLP


## Stanford CoreNLP
<p>Stanford CoreNLP provides a set of human language technology tools. It can give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, mark up the structure of sentences in terms of phrases and syntactic dependencies, indicate which noun phrases refer to the same entities, indicate sentiment, extract particular or open-class relations between entity mentions, get the quotes people said, etc.</p>
<p>Choose Stanford CoreNLP if you need:</p>
<ul>
<li>An integrated NLP toolkit with a broad range of grammatical analysis tools</li>
<li>A fast, robust annotator for arbitrary texts, widely used in production</li>
<li>A modern, regularly updated package, with the overall highest quality text analytics</li>
<li>Support for a number of major (human) languages</li>
<li>Available APIs for most major modern programming languages</li>
<li>Ability to run as a simple web service</li>
</ul>

Item | Value 
----- | -----
**SBB License** | GNU General Public License (GPL) 3.0
**Core Technology** | Java
**Project URL** | [https://stanfordnlp.github.io/CoreNLP/](https://stanfordnlp.github.io/CoreNLP/)
**Source Location** | [https://github.com/stanfordnlp/CoreNLP](https://github.com/stanfordnlp/CoreNLP)
*Tag(s)* |NLP


## Sumeval
<p><span class="col-11 text-gray-dark mr-2">Well tested &#38; Multi-language evaluation framework for text summarization. Multi-language.<br />
</span></p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [https://github.com/chakki-works/sumeval](https://github.com/chakki-works/sumeval)
**Source Location** | [https://github.com/chakki-works/sumeval](https://github.com/chakki-works/sumeval)
*Tag(s)* |NLP, Python


## Texar-PyTorch

<p><strong>Texar-PyTorch</strong> is a toolkit aiming to support a broad
 set of machine learning, especially natural language processing and 
text generation tasks. Texar provides a library of easy-to-use ML 
modules and functionalities for composing whatever models and 
algorithms. The tool is designed for both researchers and practitioners 
for fast prototyping and experimentation.</p>



<p>Texar-PyTorch integrates many of the best features of TensorFlow into
 PyTorch, delivering highly usable and customizable modules superior to 
PyTorch native ones.</p>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [https://asyml.io/](https://asyml.io/)
**Source Location** | [https://github.com/asyml/texar-pytorch](https://github.com/asyml/texar-pytorch)
*Tag(s)* |ML, NLP, Python


## TextBlob: Simplified Text Processing
<p><em>TextBlob</em> is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.</p>
<h2>Features</h2>
<ul>
<li>Noun phrase extraction</li>
<li>Part-of-speech tagging</li>
<li>Sentiment analysis</li>
<li>Classification (Naive Bayes, Decision Tree)</li>
<li>Language translation and detection powered by Google Translate</li>
<li>Tokenization (splitting text into words and sentences)</li>
<li>Word and phrase frequencies</li>
<li>Parsing</li>
<li>n-grams</li>
<li>Word inflection (pluralization and singularization) and lemmatization</li>
<li>Spelling correction</li>
<li>Add new models or languages through extensions</li>
<li>WordNet integration</li>
</ul>


<p></p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://textblob.readthedocs.io/en/dev/](https://textblob.readthedocs.io/en/dev/)
**Source Location** | [https://github.com/sloria/textblob](https://github.com/sloria/textblob)
*Tag(s)* |NLP, Python


## Thinc

<p>Thinc is the machine learning library powering spaCy. It features a battle-tested linear model designed for large sparse learning problems, and a flexible neural network model under development for spaCy v2.0.</p>



<p>Thinc is a lightweight deep learning library that offers an elegant, type-checked, functional-programming API for composing models, with support for layers defined in other frameworks such as PyTorch, TensorFlow and MXNet. You can use Thinc as an interface layer, a standalone toolkit or a flexible way to develop new models.</p>



<p>Thinc is a practical toolkit for implementing models that follow the &#8220;Embed, encode, attend, predict&#8221; architecture. It&#8217;s designed to be easy to install, efficient for CPU usage and optimised for NLP and deep learning with text â€“ in particular, hierarchically structured input and variable-length sequences.</p>

Item | Value 
----- | -----
**SBB License** | MIT License
**Core Technology** | Python
**Project URL** | [https://thinc.ai/](https://thinc.ai/)
**Source Location** | [https://github.com/explosion/thinc](https://github.com/explosion/thinc)
*Tag(s)* |ML, ML Framework, NLP, Python


## Torchtext
<p><span class="col-11 text-gray-dark mr-2">Data loaders and abstractions for text and NLP. Build on PyTorch.</span></p>
<p>&#160;</p>

Item | Value 
----- | -----
**SBB License** | BSD License 2.0 (3-clause, New or Revised) License
**Core Technology** | 
**Project URL** | [https://github.com/pytorch/text](https://github.com/pytorch/text)
**Source Location** | [https://github.com/pytorch/text](https://github.com/pytorch/text)
*Tag(s)* |NLP


## Transformers

<p>Transformers (formerly known as <code>pytorch-transformers</code> and <code>pytorch-pretrained-bert</code>)
 provides state-of-the-art general-purpose architectures (BERT, GPT-2, 
RoBERTa, XLM, DistilBert, XLNet&#8230;) for Natural Language Understanding 
(NLU) and Natural Language Generation (NLG) with over 32+ pretrained 
models in 100+ languages and deep interoperability between TensorFlow 
2.0 and PyTorch.</p>



<h3><a href="https://github.com/huggingface/transformers#features"></a></h3>



<p>Features</p>



<ul><li>As easy to use as pytorch-transformers</li><li>As powerful and concise as Keras</li><li>High performance on NLU and NLG tasks</li><li>Low barrier to entry for educators and practitioners</li></ul>



<p>State-of-the-art NLP for everyone:</p>



<ul><li>Deep learning researchers</li><li>Hands-on practitioners</li><li>AI/ML/NLP teachers and educators</li></ul>



<p>Lower compute costs, smaller carbon footprint</p>



<ul><li>Researchers can share trained models instead of always retraining</li><li>Practitioners can reduce compute time and production costs</li><li>8 architectures with over 30 pretrained models, some in more than 100 languages</li></ul>

Item | Value 
----- | -----
**SBB License** | Apache License 2.0
**Core Technology** | Python
**Project URL** | [https://huggingface.co/transformers/](https://huggingface.co/transformers/)
**Source Location** | [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)
*Tag(s)* |NLP, Python

End of SBB list <br>