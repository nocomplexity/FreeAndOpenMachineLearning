**AllenNLP** 
^^^^^^^^^^^^^^
**SBB Description:** An open-source NLP research library, built on PyTorch. AllenNLP is a NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks. AllenNLP makes it easy to design and evaluate new deep learning models for nearly any NLP problem, along with the infrastructure to easily run them in the cloud or on your laptop.

AllenNLP was designed with the following principles:

Hyper-modular and lightweight. Use the parts which you like seamlessly with PyTorch.
Extensively tested and easy to extend. Test coverage is above 90% and the example models provide a template for contributions.
Take padding and masking seriously, making it easy to implement correct models without the pain.
Experiment friendly. Run reproducible experiments from a json specification with comprehensive logging.

**SBB License:** Apache License 2.0

**Core Technology:** Python

**Project URL:** http://allennlp.org/

**Source Location:** https://github.com/allenai/allennlp





**Apache OpenNLP** 
^^^^^^^^^^^^^^^^^^^^
**SBB Description:** The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.
The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text. It supports the most common NLP tasks, such as tokenization, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, and coreference resolution. These tasks are usually required to build more advanced text processing services. OpenNLP also included maximum entropy and perceptron based machine learning.
The goal of the OpenNLP project will be to create a mature toolkit for the abovementioned tasks. An additional goal is to provide a large number of pre-built models for a variety of languages, as well as the annotated text resources that those models are derived from.

**SBB License:** Apache License 2.0

**Core Technology:** Java

**Project URL:** http://opennlp.apache.org/

**Source Location:** http://opennlp.apache.org/source-code.html





**Apache Tika** 
^^^^^^^^^^^^^^^^^
**SBB Description:** The Apache Tika™ toolkit detects and extracts metadata and text from over a thousand different file types (such as PPT, XLS, and PDF). All of these file types can be parsed through a single interface, making Tika useful for search engine indexing, content analysis, translation, and much more.
Several wrappers are available to use Tika in another programming language, such as Julia or Python

**SBB License:** Apache License 2.0

**Core Technology:** Java

**Project URL:** https://tika.apache.org/

**Source Location:** https://tika.apache.org/





**Gensim** 
^^^^^^^^^^^^
**SBB Description:** Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.
&#160;

**SBB License:** MIT License

**Core Technology:** Python

**Project URL:** https://github.com/RaRe-Technologies/gensim

**Source Location:** https://github.com/RaRe-Technologies/gensim





**Neuralcoref** 
^^^^^^^^^^^^^^^^^
**SBB Description:** State-of-the-art coreference resolution based on neural nets and spaCy.
NeuralCoref is a pipeline extension for spaCy 2.0 that annotates and resolves coreference clusters using a neural network. NeuralCoref is production-ready, integrated in spaCy&#8217;s NLP pipeline and easily extensible to new training datasets.

**SBB License:** MIT License

**Core Technology:** Python

**Project URL:** https://huggingface.co/coref/

**Source Location:** https://github.com/huggingface/neuralcoref





**NLP Architect** 
^^^^^^^^^^^^^^^^^^^
**SBB Description:** NLP Architect is an open-source Python library for exploring the state-of-the-art deep learning topologies and techniques for natural language processing and natural language understanding. It is intended to be a platform for future research and collaboration.

How can NLP Architect be used:

Train models using provided algorithms, reference datasets and configurations
Train models using your own data
Create new/extend models based on existing models or topologies
Explore how deep learning models tackle various NLP tasks
Experiment and optimize state-of-the-art deep learning algorithms
integrate modules and utilities from the library to solutions

**SBB License:** Apache License 2.0

**Core Technology:** Python

**Project URL:** http://nlp_architect.nervanasys.com/

**Source Location:** https://github.com/NervanaSystems/nlp-architect





**NLTK (Natural Language Toolkit)** 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
**SBB Description:** NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.
Check also the (free) online Book (OReily published)

**SBB License:** Apache License 2.0

**Core Technology:** Python

**Project URL:** http://www.nltk.org

**Source Location:** https://github.com/nltk/nltk





**Pattern** 
^^^^^^^^^^^^^
**SBB Description:** Pattern is a web mining module for Python. It has tools for:

Data Mining: web services (Google, Twitter, Wikipedia), web crawler, HTML DOM parser
Natural Language Processing: part-of-speech taggers, n-gram search, sentiment analysis, WordNet
Machine Learning: vector space model, clustering, classification (KNN, SVM, Perceptron)
Network Analysis: graph centrality and visualization.

**SBB License:** BSD License 2.0 (3-clause, New or Revised) License

**Core Technology:** Python

**Project URL:** https://www.clips.uantwerpen.be/pages/pattern

**Source Location:** https://github.com/clips/pattern





**PDFx** 
^^^^^^^^^^
**SBB Description:** Extract references (pdf, url, doi, arxiv) and metadata from a PDF. Optionally download all referenced PDFs and check for broken links.
Features

Extract references and metadata from a given PDF
Detects pdf, url, arxiv and doi references
Fast, parallel download of all referenced PDFs
Find broken hyperlinks (using the -c flag) (more)
Output as text or JSON (using the -j flag)
Extract the PDF text (using the --text flag)
Use as command-line tool or Python package
Compatible with Python 2 and 3
Works with local and online pdfs

**SBB License:** Apache License 2.0

**Core Technology:** Python

**Project URL:** https://www.metachris.com/pdfx/

**Source Location:** https://github.com/metachris/pdfx





**SpaCy** 
^^^^^^^^^^^
**SBB Description:** Industrial-strength Natural Language Processing (NLP) with Python and Cython
Features:

Non-destructive tokenization
Named entity recognition
Support for 26+ languages
13 statistical models for 8 languages
Pre-trained word vectors
Easy deep learning integration
Part-of-speech tagging
Labelled dependency parsing
Syntax-driven sentence segmentation
Built in visualizers for syntax and NER
Convenient string-to-hash mapping
Export to numpy data arrays
Efficient binary serialization
Easy model packaging and deployment
State-of-the-art speed
Robust, rigorously evaluated accuracy

**SBB License:** MIT License

**Core Technology:** Python

**Project URL:** https://spacy.io/

**Source Location:** https://github.com/explosion/spaCy





**Stanford CoreNLP** 
^^^^^^^^^^^^^^^^^^^^^^
**SBB Description:** Stanford CoreNLP provides a set of human language technology tools. It can give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, mark up the structure of sentences in terms of phrases and syntactic dependencies, indicate which noun phrases refer to the same entities, indicate sentiment, extract particular or open-class relations between entity mentions, get the quotes people said, etc.
Choose Stanford CoreNLP if you need:

An integrated NLP toolkit with a broad range of grammatical analysis tools
A fast, robust annotator for arbitrary texts, widely used in production
A modern, regularly updated package, with the overall highest quality text analytics
Support for a number of major (human) languages
Available APIs for most major modern programming languages
Ability to run as a simple web service

**SBB License:** GNU General Public License (GPL) 3.0

**Core Technology:** Java

**Project URL:** https://stanfordnlp.github.io/CoreNLP/

**Source Location:** https://github.com/stanfordnlp/CoreNLP





**Sumeval** 
^^^^^^^^^^^^^
**SBB Description:** Well tested &#38; Multi-language evaluation framework for text summarization.

**SBB License:** Apache License 2.0

**Core Technology:** Python

**Project URL:** https://github.com/chakki-works/sumeval

**Source Location:** https://github.com/chakki-works/sumeval





**TextBlob: Simplified Text Processing** 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
**SBB Description:** TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.
Features

Noun phrase extraction
Part-of-speech tagging
Sentiment analysis
Classification (Naive Bayes, Decision Tree)
Language translation and detection powered by Google Translate
Tokenization (splitting text into words and sentences)
Word and phrase frequencies
Parsing
n-grams
Word inflection (pluralization and singularization) and lemmatization
Spelling correction
Add new models or languages through extensions
WordNet integration

**SBB License:** MIT License

**Core Technology:** Python

**Project URL:** https://textblob.readthedocs.io/en/dev/

**Source Location:** https://github.com/sloria/textblob





**Thinc** 
^^^^^^^^^^^
**SBB Description:** Thinc is the machine learning library powering spaCy. It features a battle-tested linear model designed for large sparse learning problems, and a flexible neural network model under development for spaCy v2.0.
Thinc is a practical toolkit for implementing models that follow the &#8220;Embed, encode, attend, predict&#8221; architecture. It&#8217;s designed to be easy to install, efficient for CPU usage and optimised for NLP and deep learning with text – in particular, hierarchically structured input and variable-length sequences.

**SBB License:** GNU General Public License (GPL) 2.0

**Core Technology:** Python

**Project URL:** https://explosion.ai/

**Source Location:** https://github.com/explosion/thinc





**Torchtext** 
^^^^^^^^^^^^^^^
**SBB Description:** Data loaders and abstractions for text and NLP. Build on PyTorch.
&#160;

**SBB License:** BSD License 2.0 (3-clause, New or Revised) License

**Core Technology:** 

**Project URL:** https://github.com/pytorch/text

**Source Location:** https://github.com/pytorch/text





End of SBB list <br>